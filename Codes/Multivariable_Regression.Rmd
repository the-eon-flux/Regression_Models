---
title: "Multivariable_Regression_Notes"
author: "Tejus"
date: "17/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(car)
```

## The linear model  

* The general linear model extends simple linear regression (SLR)
by adding terms linearly into the model.  
      $Y_i = \beta_1 X_{1i} + \beta_2 X_{2i} + \ldots + \beta_{p} X_{pi} + \epsilon_{i} = \sum_{k=1}^p X_{ik} \beta_j + \epsilon_{i}$  
        
* Here $X_{1i}=1$ typically, so that an intercept is included.  
* Least squares (and hence ML estimates under iid Gaussianity 
of the errors) minimizes  
$\sum_{i=1}^n \left(Y_i - \sum_{k=1}^p X_{ki} \beta_j\right)^2$
* Note, the important linearity is linearity in the coefficients.
Thus
$Y_i =  \beta_1 X_{1i}^2 + \beta_2 X_{2i}^2 + \ldots + \beta_{p} X_{pi}^2 + \epsilon_{i}$ is still a linear model. (We've just squared the elements of the predictor variables.)

---
## The general case
* Least squares solutions have to minimize
$\sum_{i=1}^n (Y_i - X_{1i}\beta_1 - \ldots - X_{pi}\beta_p)^2$  

* The least squares estimate for the coefficient of a multivariate regression model is exactly regression through the origin with the linear relationships with the other regressors removed from both the regressor and outcome by taking residuals.  
* In this sense, multivariate regression "adjusts" a coefficient for the linear impact of the other variables.  

---
## Demonstration that it works using an example
### Linear model with two variables

```{r results='hide'}
n = 100; x = rnorm(n); x2 = rnorm(n); x3 = rnorm(n)
y = 1 + x + x2 + x3 + rnorm(n, sd = .1)

ey = resid(lm(y ~ x2 + x3)) # Linear effect of X2 & X3 removed from Y
ex = resid(lm(x ~ x2 + x3)) # Linear effect of X2 & X3 removed from X1

# Slope of X1; beta_1 = 
sum(ey * ex) / sum(ex ^ 2) # Coef beta_1 formula
coef(lm(ey ~ ex - 1)) # Same value as above
# 'Regression through the origin (-1 ) with the linear relationships with the other regressors (X2 &X3 here) removed from both the regressor (X1) and outcome (Y) by taking residuals.'

# Again repeating
coef(lm(y ~ x + x2 + x3)) # Coef of X has value same as when effect of other regressors were removed from Y & X (by taking their residuals) and doing a lm through the origin.

```

---
## Fitted values, residuals and residual variation
All of our SLR quantities can be extended to linear models  

* Model $Y_i = \sum_{k=1}^p X_{ik} \beta_{k} + \epsilon_{i}$ where $\epsilon_i \sim N(0, \sigma^2)$
* Fitted responses $\hat Y_i = \sum_{k=1}^p X_{ik} \hat \beta_{k}$
* Residuals $e_i = Y_i - \hat Y_i$
* Variance estimate $\hat \sigma^2 = \frac{1}{n-p} \sum_{i=1}^n e_i ^2$
* To get predicted responses at new values, $x_1, \ldots, x_p$, simply plug them into the linear model $\sum_{k=1}^p x_{k} \hat \beta_{k}$
* Coefficients have standard errors, $\hat \sigma_{\hat \beta_k}$, and
$\frac{\hat \beta_k - \beta_k}{\hat \sigma_{\hat \beta_k}}$
follows a $T$ distribution with $n-p$ degrees of freedom.
* Predicted responses have standard errors and we can calculate predicted and expected response intervals.

---

## Variance inflation and Factors

* In modeling, our interest lies in parsimonious, interpretable representations of the data that enhance our understanding of the phenomena under study.
* Omitting variables results in bias in the coefficients of interest - unless their regressors are uncorrelated with the omitted ones.
* On the other hand, including any new variables increases (actual, not estimated) standard errors of other regressors. (So we don't want to idly throw variables into the model. )
* This lesson is about the 2nd problem, **Variance Inflation**.

### First Simulation

* As you can see from the below code y depends on x variable only plus some random noise.
* We will be looking at the coefficient of x1 as we keep adding new regressors to the model.

```{r Fit_lms, }
makelms <- function(x1, x2, x3){
  # Simulate a dependent variable, y, as x1 plus a normally distributed error of mean 0 and standard deviation .3.
  y <- x1 + rnorm(length(x1), sd = .3)
  # Find the coefficient of x1 in 3 nested linear models, the first including only the predictor x1, the second x1 and x2, the third x1, x2, and x3.
  c(coef(lm(y ~ x1))[2], 
    coef(lm(y ~ x1 + x2))[2], 
    coef(lm(y ~ x1 + x2 + x3))[2])
}

```

* We also have 2 additional functions (rgp1() & rgp2()) for generating those x variables as follows.
* We can see the differences between these 2 functions as follows :
      # x1, x2, and x3 are uncorrelated in rgp1()
      # Also in rgp2(), x3 is heavily correlated to x1 compared to x2.

```{r rgp_Fn}

# Regressor generation process 1.
rgp1 <- function(){
  print("Processing. Please wait.")
  # number of samples per simulation
  n <- 100
  # number of simulations
  nosim <- 1000
  # set seed for reproducibility
  set.seed(4321)
  # Point A
  x1 <- rnorm(n)
  x2 <- rnorm(n)
  x3 <- rnorm(n)
  # Point B
  betas <- sapply(1 : nosim, function(i)makelms(x1, x2, x3))
  round(apply(betas, 1, var), 5)
}

# Regressor generation process 2.
rgp2 <- function(){
  print("Processing. Please wait.")
  # number of samples per simulation
  n <- 100
  # number of simulations
  nosim <- 1000
  # set seed for reproducibility
  set.seed(4321)
  # Point C
  x1 <- rnorm(n)
  x2 <- x1/sqrt(2) + rnorm(n) /sqrt(2)
  x3 <- x1 * 0.95 + rnorm(n) * sqrt(1 - 0.95^2)
  # Point D
  betas <- sapply(1 : nosim, function(i)makelms(x1, x2, x3))
  round(apply(betas, 1, var), 5)
}


```


* Also these functions call above `makelms()` function, for `1000` simulations. Each time it generates a new dependent variable, y, and returns estimates of the coefficient of x1 for each of the 3 models, y ~ x1, y ~ x1 + x2, and y ~ x1 + x2 + x3.

* Thus our `betas` variable is an array of `1000 columns x 3 rows` dimension. In the next line with apply we then compute the variance for each row.

* We are computing variance in estimates of the coefficient of x1 in each of the three models, y ~ x1, y ~ x1 + x2, and y ~ x1 + x2 + x3.
* Now when we run first function `rgp()`, the simulation approximates the variance (i.e., squared standard error) of x1's coefficient in each of these three models. Recall that rgp1() the regressors are uncorrelated & that variance inflation is due to correlated regressors.

```{r, echo=FALSE, cache=TRUE}
print(rgp1())
```

* The variances in each of the three models are approximately equal, as expected.
* In `rgp2()`, it's a different case since x2 & x3 depend on the regressor of interest, x1. So we should expect an effect.

```{r, echo=FALSE, cache=TRUE}
print(rgp2())
```

* Variance inflation due to correlated regressors is clear, and is most pronounced in the third model, y ~ x1 + x2 + x3, since x3 is the regressor most strongly correlated with x1.  

* In a real case, we have only one set of coefficients and we depend on theoretical estimates. However, theoretical estimates contain an unknown constant of proportionality. We therefore depend on ratios of theoretical estimates called **Variance Inflation Factors, or VIFs**.


## A variance inflation factor (VIF)

* **A variance inflation factor (VIF) is a ratio of estimated variances, the variance due to including the ith regressor, divided by that due to including a corresponding ideal regressor which is uncorrelated with the others.**

* You can also say that VIF describes the increase in the variance of a coefficient due to the correlation of its regressor with the other regressors.

* Let's use the swiss dataset and see that.

* About the data : the Swiss data set consists of a standardized fertility measure and socioeconomic indicators for each of 47 French-speaking provinces of Switzerland in about 1888 when Swiss fertility rates began to fall. 
* Fertility was thought to depend on five socioeconomic factors: 
  # the percent of males working in Agriculture
  # the percent of draftees receiving the highest grade on the army's Examination
  # the percent of draftees with Education beyond primary school
  # the percent of the population which was Roman Catholic &
  # the rate of Infant Mortality in the province.

* Let's model the response term *Fertility* in terms of these five regressors and an intercept.

```{r }
data(swiss)
mdl <- lm(Fertility~. , swiss )

# Calculate the VIF's for each of the regressors
vif(mdl)
```

* These VIF's show, for each regression coefficient, the variance inflation due to including all the others.

* For instance, the variance estimated in the Education is `2.774943` times what it might have been if Education were not correlated with other regressors.

* Since Education and score on an Examination are likely to be correlated, we might guess that most of the variance inflation for Education is due to including Examination. Let's test this theory.

```{r}
mdl2 <- lm(Fertility ~ Agriculture + Education + Catholic + Infant.Mortality, swiss)
vif(mdl2)
```

* As expected, omitting Examination has markedly decreased the VIF for Education, from `2.774943` to `1.816361`.

* Note that omitting Examination has had almost no effect the VIF for Infant Mortality. 

## Review

* VIF is the square of standard error inflation.

* If a regressor is strongly correlated with others, hence will increase their VIF's, we also cannot simply exclude it because excluding it might bias coefficient estimates of regressors with which it is correlated.

* The problems of variance inflation and bias due to excluded regressors both involve correlated regressors. However there are methods, such as factor analysis or principal componenent analysis, which can convert regressors to an equivalent uncorrelated set. Keep in mind that using converted regressors may make interpretation difficult.

## Overfitting & underfitting


















