---
title: "Multivariable_Regression_Notes"
author: "Tejus"
date: "17/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(car)
```

## The linear model  

* The general linear model extends simple linear regression (SLR)
by adding terms linearly into the model.  
      $Y_i = \beta_1 X_{1i} + \beta_2 X_{2i} + \ldots + \beta_{p} X_{pi} + \epsilon_{i} = \sum_{k=1}^p X_{ik} \beta_j + \epsilon_{i}$  
        
* Here $X_{1i}=1$ typically, so that an intercept is included.  
* Least squares (and hence ML estimates under iid Gaussianity 
of the errors) minimizes  
$\sum_{i=1}^n \left(Y_i - \sum_{k=1}^p X_{ki} \beta_j\right)^2$
* Note, the important linearity is linearity in the coefficients.
Thus
$Y_i =  \beta_1 X_{1i}^2 + \beta_2 X_{2i}^2 + \ldots + \beta_{p} X_{pi}^2 + \epsilon_{i}$ is still a linear model. (We've just squared the elements of the predictor variables.)

---
## The general case
* Least squares solutions have to minimize
$\sum_{i=1}^n (Y_i - X_{1i}\beta_1 - \ldots - X_{pi}\beta_p)^2$  

* The least squares estimate for the coefficient of a multivariate regression model is exactly regression through the origin with the linear relationships with the other regressors removed from both the regressor and outcome by taking residuals.  
* In this sense, multivariate regression "adjusts" a coefficient for the linear impact of the other variables.  

---
## Demonstration that it works using an example
### Linear model with two variables

```{r results='hide'}
n = 100; x = rnorm(n); x2 = rnorm(n); x3 = rnorm(n)
y = 1 + x + x2 + x3 + rnorm(n, sd = .1)

ey = resid(lm(y ~ x2 + x3)) # Linear effect of X2 & X3 removed from Y
ex = resid(lm(x ~ x2 + x3)) # Linear effect of X2 & X3 removed from X1

# Slope of X1; beta_1 = 
sum(ey * ex) / sum(ex ^ 2) # Coef beta_1 formula
coef(lm(ey ~ ex - 1)) # Same value as above
# 'Regression through the origin (-1 ) with the linear relationships with the other regressors (X2 &X3 here) removed from both the regressor (X1) and outcome (Y) by taking residuals.'

# Again repeating
coef(lm(y ~ x + x2 + x3)) # Coef of X has value same as when effect of other regressors were removed from Y & X (by taking their residuals) and doing a lm through the origin.

```

---
## Fitted values, residuals and residual variation
All of our SLR quantities can be extended to linear models  

* Model $Y_i = \sum_{k=1}^p X_{ik} \beta_{k} + \epsilon_{i}$ where $\epsilon_i \sim N(0, \sigma^2)$
* Fitted responses $\hat Y_i = \sum_{k=1}^p X_{ik} \hat \beta_{k}$
* Residuals $e_i = Y_i - \hat Y_i$
* Variance estimate $\hat \sigma^2 = \frac{1}{n-p} \sum_{i=1}^n e_i ^2$
* To get predicted responses at new values, $x_1, \ldots, x_p$, simply plug them into the linear model $\sum_{k=1}^p x_{k} \hat \beta_{k}$
* Coefficients have standard errors, $\hat \sigma_{\hat \beta_k}$, and
$\frac{\hat \beta_k - \beta_k}{\hat \sigma_{\hat \beta_k}}$
follows a $T$ distribution with $n-p$ degrees of freedom.
* Predicted responses have standard errors and we can calculate predicted and expected response intervals.

---

## Variance inflation and Factors

### Some background

* In modeling, our interest lies in parsimonious, interpretable representations of the data that enhance our understanding of the phenomena under study.
* Omitting variables results in bias in the coefficients of interest - unless their regressors are uncorrelated with the omitted ones.
* On the other hand, including any new variables increases (actual, not estimated) standard errors of other regressors. (So we don't want to idly throw variables into the model. )
* First let us look at the 2nd problem, **Variance Inflation**.

### 1.) A variance inflation factor (VIF)

* As you can see from the below code y depends on x variable only plus some random noise.
* We will be looking at the coefficient of x1 as we keep adding new regressors to the model.

```{r Fit_lms, }
makelms <- function(x1, x2, x3){
  # Simulate a dependent variable, y, as x1 plus a normally distributed error of mean 0 and standard deviation .3.
  y <- x1 + rnorm(length(x1), sd = .3)
  # Find the coefficient of x1 in 3 nested linear models, the first including only the predictor x1, the second x1 and x2, the third x1, x2, and x3.
  c(coef(lm(y ~ x1))[2], 
    coef(lm(y ~ x1 + x2))[2], 
    coef(lm(y ~ x1 + x2 + x3))[2])
}

```

* We also have 2 additional functions (rgp1() & rgp2()) for generating those x variables as follows.
* We can see the differences between these 2 functions as follows :
      # x1, x2, and x3 are uncorrelated in rgp1()
      # Also in rgp2(), x3 is heavily correlated to x1 compared to x2.

```{r rgp_Fn}

# Regressor generation process 1.
rgp1 <- function(){
  print("Processing. Please wait.")
  # number of samples per simulation
  n <- 100
  # number of simulations
  nosim <- 1000
  # set seed for reproducibility
  set.seed(4321)
  # Point A
  x1 <- rnorm(n)
  x2 <- rnorm(n)
  x3 <- rnorm(n)
  # Point B
  betas <- sapply(1 : nosim, function(i)makelms(x1, x2, x3))
  round(apply(betas, 1, var), 5)
}

# Regressor generation process 2.
rgp2 <- function(){
  print("Processing. Please wait.")
  # number of samples per simulation
  n <- 100
  # number of simulations
  nosim <- 1000
  # set seed for reproducibility
  set.seed(4321)
  # Point C
  x1 <- rnorm(n)
  x2 <- x1/sqrt(2) + rnorm(n) /sqrt(2)
  x3 <- x1 * 0.95 + rnorm(n) * sqrt(1 - 0.95^2)
  # Point D
  betas <- sapply(1 : nosim, function(i)makelms(x1, x2, x3))
  round(apply(betas, 1, var), 5)
}


```


* Also these functions call above `makelms()` function, for `1000` simulations. Each time it generates a new dependent variable, y, and returns estimates of the coefficient of x1 for each of the 3 models, y ~ x1, y ~ x1 + x2, and y ~ x1 + x2 + x3.

* Thus our `betas` variable is an array of `1000 columns x 3 rows` dimension. In the next line with apply we then compute the variance for each row.

* We are computing variance in estimates of the coefficient of x1 in each of the three models, y ~ x1, y ~ x1 + x2, and y ~ x1 + x2 + x3.
* Now when we run first function `rgp()`, the simulation approximates the variance (i.e., squared standard error) of x1's coefficient in each of these three models. Recall that rgp1() the regressors are uncorrelated & that variance inflation is due to correlated regressors.

```{r, echo=FALSE, cache=TRUE}
print(rgp1())
```

* The variances in each of the three models are approximately equal, as expected.
* In `rgp2()`, it's a different case since x2 & x3 depend on the regressor of interest, x1. So we should expect an effect.

```{r, echo=FALSE, cache=TRUE}
print(rgp2())
```

* Variance inflation due to correlated regressors is clear, and is most pronounced in the third model, y ~ x1 + x2 + x3, since x3 is the regressor most strongly correlated with x1.  

* In a real case, we have only one set of coefficients and we depend on theoretical estimates. However, theoretical estimates contain an unknown constant of proportionality. We therefore depend on ratios of theoretical estimates called **Variance Inflation Factors, or VIFs**.


#### Definition

* **A variance inflation factor (VIF) is a ratio of estimated variances, the variance due to including the ith regressor, divided by that due to including a corresponding ideal regressor which is uncorrelated with the others.**

* You can also say that VIF describes the increase in the variance of a coefficient of the other correlated regressors.

* Let's use the swiss dataset and see that.

* About the data : the Swiss data set consists of a standardized fertility measure and socioeconomic indicators for each of 47 French-speaking provinces of Switzerland in about 1888 when Swiss fertility rates began to fall. 
* Fertility was thought to depend on five socioeconomic factors: 
  # the percent of males working in Agriculture
  # the percent of draftees receiving the highest grade on the army's Examination
  # the percent of draftees with Education beyond primary school
  # the percent of the population which was Roman Catholic &
  # the rate of Infant Mortality in the province.

* Let's model the response term *Fertility* in terms of these five regressors and an intercept.

```{r }
data(swiss)
mdl <- lm(Fertility~. , swiss )

# Calculate the VIF's for each of the regressors
vif(mdl)
```

* These VIF's show, for each regression coefficient, the variance inflation due to including all the others.

* For instance, the variance estimated in the Education is `2.774943` times what it might have been if Education were not correlated with other regressors.

* Since Education and score on an Examination are likely to be correlated, we might guess that most of the variance inflation for Education is due to including Examination. Let's test this theory.

```{r}
mdl2 <- lm(Fertility ~ Agriculture + Education + Catholic + Infant.Mortality, swiss)
vif(mdl2)
```

* As expected, omitting Examination has markedly decreased the VIF for Education, from `2.774943` to `1.816361`.

* Note that omitting Examination has had almost no effect the VIF for Infant Mortality. 

#### Review

* VIF is the square of standard error inflation.

* If a regressor is strongly correlated with others, hence will increase their VIF's, we also cannot simply exclude it because excluding it might bias coefficient estimates of regressors with which it is correlated.

* The problems of variance inflation and bias due to excluded regressors both involve correlated regressors. However there are methods, such as factor analysis or principal componenent analysis, which can convert regressors to an equivalent uncorrelated set. Keep in mind that using converted regressors may make interpretation difficult.

### 2.) Overfitting & underfitting

* Let us look at the effect of omitted variables and discuss the use of ANOVA to construct parsimonious, interpretable representations of the data.

* First let's see how omitting a correlated regressor can bias estimates of a coefficient.

* Look at the following simulation function `simbias()`

```{r simBias}
simbias <- function(seed=8765){
  # The default seed guarantees a nice histogram. This is the only
  # reason that accepting the default, x1c <- simbias(), is required in the lesson. 
  # The effect will be evident with other seeds as well.
  set.seed(seed) 
  temp <- rnorm(100)
  # Point A
  x1 <- (temp + rnorm(100))/sqrt(2)
  x2 <- (temp + rnorm(100))/sqrt(2)
  x3 <- rnorm(100)
  # Function to simulate regression of y on 2 variables.
  f <- function(k){
    # Point B
    y <- x1 + x2 + x3 + .3*rnorm(100)
    # Point C
    c(lm(y ~ x1 + x2)$coef[2],
       lm(y ~ x1 + x3)$coef[2])
  }
  # Point D
  sapply(1:150, f)
}

```

* We have 2 vars correlated `x1 & x2` and notice the Fn within `simbias()` which simulates `150` y variables and returns the coef of x1 for the 2 models :
  # lm(y~x1 + x2) `Our correlated variables` &
  # lm(y ~ x1 + x3) `Uncorrelated variables`

* As you can see from the `point B` comment the actual coef of `x1` is `1`.

* Results retuned from simulation matrix are as a 2x150 matrix.  

* The first row of this matrix contains independent estimates of x1's coefficient in the case that x3, the regressor uncorrelated with x1, is omitted.

* The second row contains estimates of x1's coefficient when the correlated regressor, x2, is omitted.

* Let's run the code and see the results.

```{r }
x1c <- simbias()
```

* We know as we have omiited the correlated variable, we would expect the mean estimate of x1c's in 2nd row of results **to be farther from 1** than the mean of x1c's in first row.  

```{r}
apply(x1c, 1, mean)
```

* As expected the 2nd value is greater than the first. Let's plot it and see the differences.  

```{r}

# Plot histograms illustrating bias in estimates of a regressor
# coefficient 1) when an uncorrelated regressor is missing and
# 2) when a correlated regressor is missing.
x1hist <- function(x1c){
  p1 <- hist(x1c[1,], plot=FALSE)
  p2 <- hist(x1c[2,], plot=FALSE)
  yrange <- c(0, max(p1$counts, p2$counts))
  plot(p1, col=rgb(0,0,1,1/4), xlim=range(x1c), ylim=yrange, xlab="Estimated coefficient of x1",
        main="Bias Effect of Omitted Regressor")
  plot(p2, col=rgb(1,0,0,1/4), xlim=range(x1c), ylim=yrange, add=TRUE)
  legend(1.1, 40, c("Uncorrelated regressor, x3, omitted", "Correlated regressor, x2, omitted"),
         fill=c(rgb(0,0,1,1/4), rgb(1,0,0,1/4)))
}
x1hist(x1c)

```

* Histograms of estimates from x1c's first row (blue) and second row (red) are shown. **Estimates from the second row are clearly more than two standard deviations from the correct value of 1, and the bias due to omitting the correlated regressor is evident**.

* Adding even irrelevant regressors can cause a model to tend toward a perfect fit. We illustrate this by adding random regressors to the swiss data and regressing on progressively more of them. 

* **As the number of regressors approaches the number of data points (47), the residual sum of squares, also known as the deviance, approaches 0**.

```{r Bogus}
# Illustrate the effect of bogus regressors on residual squared error.
bogus <- function(){
  temp <- swiss
  # Add 41 columns of random regressors to a copy of the swiss data.
  for(n in 1:41){temp[,paste0("random",n)] <- rnorm(nrow(temp))}
  # Define a function to compute the deviance of Fertility regressed
  # on all regressors up to column n. The function, deviance(model), computes
  # the residual sum of squares of the model given as its argument.
  f <- function(n){deviance(lm(Fertility ~ ., temp[,1:n]))}
  # Apply f to data from n=6, i.e., the legitimate regressors,
  # through n=47, i.e., a full complement of bogus regressors.
  rss <- sapply(6:47, f)
  # Display result.
  plot(0:41, rss, xlab="Number of bogus regressors.", ylab="Residual squared error.",
       main="Residual Squared Error for Swiss Data\nUsing Irrelevant (Bogus) Regressors",
       pch=21, bg='red')
}
bogus()


```

* In the figure, adding random regressors decreased deviance, but we would be mistaken to believe that such decreases are significant.

* To assess significance, we should take into account that adding regressors reduces residual degrees of freedom.

* Analysis of variance (ANOVA) is a useful way to quantify the significance of additional regressors. To exemplify its use, we will use the swiss data.

```{r ANOVA}

# regress Fertility on Agriculture
fit1 <- lm(Fertility ~ Agriculture, swiss)

# regress Fertility on Agriculture & 2 additional params Examination & Education
fit3 <- lm(Fertility ~ Agriculture + Examination + Education , swiss)

# ANOVA
anova(fit1, fit3)
```

* We have used anova to assess the significance of the two added regressors. **The null hypothesis is that the added regressors are not significant.**

* The `***` indicates that the null hypothesis is rejected at the 0.001 level, so at least one of the two additional regressors is significant.

* Rejection is based on a right-tailed F test, Pr(>F), applied to an F value.

*  An F statistic is a ratio of two sums of squares divided by their respective degrees of freedom. If the two scaled sums are independent and centrally chi-squared distributed with the same variance, the statistic will have an F distribution with parameters given by the two degrees of freedom. In our case, the two sums are residual sums of squares which, as we know, have mean zero hence are centrally chi-squared provided the residuals themselves are normally distributed. The two relevant sums are given in the RSS (Residual Sum of Squares) column of the table.

* R's function, deviance(model), calculates the residual sum of squares (RSS in ANOVA result), also known as the deviance, of the linear model given as its argument.

#### F value (ANOVA) calculation

* Denominator : fit3's residual sum of squares / fit3's degrees of freedom.
  # Df : subtracting 4, the number of fit3's predictors (the 3 named and the intercept,) from 47
  # `d <- deviance(fit3)/(47 - 3 - 1)` 

* Numerator : The numerator is the difference, deviance(fit1)-deviance(fit3), divided by the difference in the residual degrees of freedom of fit1 and fit3, namely 2.
  # `n <- (deviance(fit1)-deviance(fit3)) / 2`

* Ratio now `n/d` is your F statistic.

#### P value calculation

* Probability that a value of `n/d` or larger would be drawn from an F distribution which has parameters 2 and 43. Given by `4.407e-07 in the column labeled Pr(>F)` in ANOVA table.

* `pf(n/d, 2, 43,lower.tail=FALSE)`; Same very small p value, so we are confident that fit3 is significantly better than fit1, with one caveat:
  # analysis of variance is sensitive to its assumption that model residuals are approximately normal. If they are not, we could get a small p-value for that reason.
  # The Shapiro-Wilk test is quick and easy in R, Normality is its null hypothesis.
  # `shapiro.test(fit3$residuals)` Since p-value is large; We accept the H0 hypothesis; supporting confidence in our ANOVA. 

## Review

* Omitting a correlated regressor can bias estimation of the coefficient of certain other regressors.

* Including more regressors will reduce a model's residual sum of squares, even if the new regressors are irrelevant. 

* When adding regressors, the reduction in residual sums of squares should be tested for significance above and beyond that of reducing residual degrees of freedom. R's anova() function uses an F-test for this purpose. Before that the **model residuals should be tested for normality.**


















