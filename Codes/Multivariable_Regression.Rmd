---
title: "Multivariable_Regression_Notes"
author: "Tejus"
date: "17/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## The linear model  

* The general linear model extends simple linear regression (SLR)
by adding terms linearly into the model.  
      $Y_i = \beta_1 X_{1i} + \beta_2 X_{2i} + \ldots + \beta_{p} X_{pi} + \epsilon_{i} = \sum_{k=1}^p X_{ik} \beta_j + \epsilon_{i}$  
        
* Here $X_{1i}=1$ typically, so that an intercept is included.  
* Least squares (and hence ML estimates under iid Gaussianity 
of the errors) minimizes  
$\sum_{i=1}^n \left(Y_i - \sum_{k=1}^p X_{ki} \beta_j\right)^2$
* Note, the important linearity is linearity in the coefficients.
Thus
$Y_i =  \beta_1 X_{1i}^2 + \beta_2 X_{2i}^2 + \ldots + \beta_{p} X_{pi}^2 + \epsilon_{i}$ is still a linear model. (We've just squared the elements of the predictor variables.)

---
## The general case
* Least squares solutions have to minimize
$\sum_{i=1}^n (Y_i - X_{1i}\beta_1 - \ldots - X_{pi}\beta_p)^2$  

* The least squares estimate for the coefficient of a multivariate regression model is exactly regression through the origin with the linear relationships with the other regressors removed from both the regressor and outcome by taking residuals.  
* In this sense, multivariate regression "adjusts" a coefficient for the linear impact of the other variables.  

---
## Demonstration that it works using an example
### Linear model with two variables

```{r results='hide'}
n = 100; x = rnorm(n); x2 = rnorm(n); x3 = rnorm(n)
y = 1 + x + x2 + x3 + rnorm(n, sd = .1)

ey = resid(lm(y ~ x2 + x3)) # Linear effect of X2 & X3 removed from Y
ex = resid(lm(x ~ x2 + x3)) # Linear effect of X2 & X3 removed from X1

# Slope of X1; beta_1 = 
sum(ey * ex) / sum(ex ^ 2) # Coef beta_1 formula
coef(lm(ey ~ ex - 1)) # Same value as above
# 'Regression through the origin (-1 ) with the linear relationships with the other regressors (X2 &X3 here) removed from both the regressor (X1) and outcome (Y) by taking residuals.'

# Again repeating
coef(lm(y ~ x + x2 + x3)) # Coef of X has value same as when effect of other regressors were removed from Y & X (by taking their residuals) and doing a lm through the origin.

```

---
## Fitted values, residuals and residual variation
All of our SLR quantities can be extended to linear models  

* Model $Y_i = \sum_{k=1}^p X_{ik} \beta_{k} + \epsilon_{i}$ where $\epsilon_i \sim N(0, \sigma^2)$
* Fitted responses $\hat Y_i = \sum_{k=1}^p X_{ik} \hat \beta_{k}$
* Residuals $e_i = Y_i - \hat Y_i$
* Variance estimate $\hat \sigma^2 = \frac{1}{n-p} \sum_{i=1}^n e_i ^2$
* To get predicted responses at new values, $x_1, \ldots, x_p$, simply plug them into the linear model $\sum_{k=1}^p x_{k} \hat \beta_{k}$
* Coefficients have standard errors, $\hat \sigma_{\hat \beta_k}$, and
$\frac{\hat \beta_k - \beta_k}{\hat \sigma_{\hat \beta_k}}$
follows a $T$ distribution with $n-p$ degrees of freedom.
* Predicted responses have standard errors and we can calculate predicted and expected response intervals.

---





























